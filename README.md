# HW7—Neural Nets

## Backstory

Pied Piper, led by Mr. Hendricks, is a company on a mission to create human-like artificial intelligence (AI). At Pied Piper, we want the development of such AI to have help from another AI that guides the former on moral values. In today’s world, AI is thought to lack consciousness, moral decision-making, and understanding of societal norms. Our goal is to help AI to understand moral and societal conduct of humans in different scenarios so as to achieve alignment with human values.

Your job is to assist Mr. Hendricks in one of our new use-case projects regarding text generation through AI. With this project, the focus is on developing a machine learning (ML) model with the ability to generate text based on logical reasoning used in everyday situations and make decisions (moral or immoral) as humans would. To that end, you need to do a few things: use relevant GPT-2 code from Hugging Face (an open-source "AI community building the future"), plot curves, and achieve the best results based on fine-tuning **hyperparameters** and **model parameters** of the ML model you develop. Based on your results, an assessment of the feasibility and the scope of the project will determine whether to move forward with it or not. 

## Description

The dataset you will use is “moral_stories” from hugging face [https://huggingface.co/datasets/demelin/moral_stories]. You will use the GPT-2 model to generate moral social conduct (*"norm"* column of the dataset). Your task is expected to help you to gain the following:

* The ability to navigate and use the Hugging Face Website and GitHub repositories
* The ability to use and integrate *"wandb"* (https://wandb.ai/site) as a logger and use plot curves
* A better understanding of neural networks (NNs) and the effect of hyperparameter changes on a NN model.

## What you need to do: 

***

![alt text](https://github.com/vikramNU/introaihw6/blob/main/images/HW6.png)

* Training: Go to the Hugging Face Website, and within the section on Transformers ([https://huggingface.co/docs/transformers/v4.21.2/en/index]), go through the modules of the Hugging Face API and GPT-2 to understand how to implement functions like *GPT2Tokenizer*, *GPT2LMHeadModel*, training arguments, etc. within the framework of code provided to you in a Jupyter Notebook. The Notebook has some code already implemented, but there are places in the code where you need to insert your implementation.

* Fine-tuning: You need to fine-tune the GPT-2 model by experimenting with training hyperparameters like *training_epoch* (within the range of 1 to 5), *batch_size* (for evaluation and training), *learning rate* and *weight decay*, and inference hyperparameters like *top_p*, *top_k*,and *temperature* when generating the model, then choose the best-fit values for your model. 

* Curving Losses: You need to plot the loss curves using *wandb* ([https://docs.wandb.ai/guides/integrations/huggingface]) to track your losses. 

* Testing on Sample Data: Test your model (by generating sentences) for a sample data of your own choice (e.g, “Parents are allowed to decide”). Remember that this will require deciphering tokens from numbers generated by your model.

* Testing on Test Set: Generate sentences using the test set.

* Evaluation: Evaluate your model using the BLEU score.

## PDF Deliverable: 

***

On Canvas, submit a PDF containing the following:

* Loss curves from *wandb* for at least 3 tuning changes for GPT-2 hyperparameters (refer to "Fine-tuning"  and "Curving Losses" above) and explain your reasoning for how the changes affect the results for each hyperparameter.
* For each of the aforementioned 3 tuning changes, report your BLEU score in a tabular format.

